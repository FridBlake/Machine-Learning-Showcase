{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMQ6zoNN5kdtjRmG9pExy3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FridBlake/Machine-Learning-Showcase/blob/main/The_Titanic_Dataset_from_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rqBQQlCdUs4"
      },
      "outputs": [],
      "source": [
        "#these two modules will help us to graph the data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#pandas will help us visualise the data and manipulate it so that we can clean and treat it.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#put this on whenver you use matplotlib\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we are strong a PANDAS object of type 'DataFrame' in a variable data, which will use a .csv file.\n",
        "data = pd.read_csv('train (1).csv')\n",
        "data"
      ],
      "metadata": {
        "id": "n8mvrXoNeKZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation\n"
      ],
      "metadata": {
        "id": "aYoCtkDUesz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#process by which missing values are replaced with the most likely ones.\n",
        "\n",
        "#.isnull() shows you a DataFrame, with T/F to show if the data is missing. .sum() shows you the sum for all missing data in a column.\n",
        "data.isnull().sum()\n",
        "\n",
        "#when you want to isolate a single column, use data[['Column name here']]. .median() finds the median of data the column.\n",
        "medianage = data['Age'].median()\n",
        "\n",
        "#.fillna() fills the missing data, taking in the 1st parameter as what you want to replace the data with, and inplace=True, if you want the original data variable to be modified.\n",
        "data['Age'].fillna(medianage, inplace=True)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "UY4ALWMDepR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in enumerate(['SibSp', 'Parch', 'Age']):\n",
        "  plt.figure(i)\n",
        "  sns.catplot(x=j, y='Survived', data= data, kind='point', aspect=2)\n",
        "\n",
        "\n",
        "for i, j in enumerate(['Sex']):\n",
        "  plt.figure(i)\n",
        "  sns.catplot(x=j, y='Survived', data= data, kind='box', aspect=2)"
      ],
      "metadata": {
        "id": "ewgaTWbljSUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''since we now see that there is a relationship (inverse) beteween the number of family members they have in total and the survival rate,\n",
        "we can go ahead and add them up into one column, so as to prevent collinearity. - Plus, we also know that age is not really following a pattern at all.'''\n",
        "\n",
        "data['fam_ct'] = data['SibSp'] + data['Parch']\n",
        "\n",
        "#now we can just go ahead and get rid of SibSp and PARCH.\n",
        "\n",
        "data.drop('SibSp', axis=1, inplace=True)\n",
        "data.drop('Parch', axis=1, inplace=True)\n",
        "data.drop('PassengerId', axis=1, inplace=True)\n",
        "\n",
        "data\n"
      ],
      "metadata": {
        "id": "-xtCYhuK2rPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I would like to go ahead and work with the categorical variables, and change them."
      ],
      "metadata": {
        "id": "F5WSTgTu3ZmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I replace sex with binary numbers.\n",
        "data['Sex'] = data['Sex'].map({'male' : 0, 'female' : 1})\n",
        "\n",
        "#Now, I would like to see if cabin is something that has been randomly missing.\n",
        "\n",
        "data.groupby(data['Cabin'].isnull())['Survived'].mean()\n",
        "\n",
        "#When I ran that the data for cabin is not missing in random. I continued on without showing the output as I edited the code block already. Those without a cabin were much more likely to survive.\n",
        "#In such a case, np.where() could be useful.\n",
        "\n",
        "#if they don't have a cabin, it's a 0, else it's a 1.\n",
        "data[['hasCabin']] = np.where(data[['Cabin']].isnull(), 0, 1)\n",
        "data.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "#now I would like to do the same thing for data['Embarked'] and see if it makes a difference at all.\n",
        "\n",
        "data.groupby(data['Embarked'])['Survived'].mean()\n",
        "\n",
        "#I don't think where they embarked WOULD have a big effect, if any it'll be NEGLIGIBLE.\n",
        "\n",
        "data\n",
        "\n"
      ],
      "metadata": {
        "id": "8blb_B2L3gfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So I drop the ones that we do not need.\n",
        "data.drop('Embarked', axis=1, inplace=True)\n",
        "data.drop('Name', axis=1, inplace=True)\n",
        "data.drop('Ticket', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "cJF5e5RY7rSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#this is what we are left with now."
      ],
      "metadata": {
        "id": "CcIkPEOB77e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data[['Survived']]\n",
        "\n",
        "cNames = X.columns\n",
        "\n",
        "#data has now been normalised\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=cNames)\n",
        "X\n",
        "y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "#now 30 percent of the data is in testing group."
      ],
      "metadata": {
        "id": "WwxW2oYf8zHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.groupby(['fam_ct'])['Survived'].median()"
      ],
      "metadata": {
        "id": "5rUCQ73ABgfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "ZihLvlhSkb_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us first try out a few different classification methods, and then go with the best.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "RC_model = RandomForestClassifier()\n",
        "\n",
        "parameters = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 'max_depth' : [1, 2, 3, 4, 5, 6]}\n",
        "\n",
        "cv = GridSearchCV(RC_model, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "95gpcY_ik1dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.best_estimator_\n",
        "#with this we can find that this random forest classifier works best at a max depth of 6 and n_est = 20."
      ],
      "metadata": {
        "id": "BmF90BQ-mlqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The random forst model has an accuracy of 87 percent.\n",
        "\n",
        "rf_Model = RandomForestClassifier(max_depth = 6, n_estimators = 20)\n",
        "rf_Model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_Model.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "8LI7Ck3Pmogh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Now, let us try a multilayer perceptron.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "nn_Model = keras.Sequential([\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cb = keras.callbacks.EarlyStopping(min_delta = 0.01, patience=3)\n",
        "\n",
        "nn_Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = nn_Model.fit(X_train, y_train, epochs=100, callbacks = cb, validation_data=(X_test, y_test), batch_size=32)\n",
        "\n",
        "plt.plot(history.history['loss'], color='blue')\n",
        "plt.plot(history.history['val_loss'], color='yellow')\n",
        "plt.show()\n",
        "\n",
        "y_pred = nn_Model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "haqFuURZnbNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "now that is only as far as i can get it to go. Neural networks are rather notorious to train indeed.\n",
        "I want to try naive_bayes, GaussianNB to be specific. - Again, Gaussian NB turns out to be terrible in this case.\n",
        "'''"
      ],
      "metadata": {
        "id": "ElqblyU7rADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_test, y_test)\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))\n"
      ],
      "metadata": {
        "id": "O6x4ybjwrTgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try out logistic regression - used for data that's rather 'behaved' - in other words, it's not made of any outliers.\n"
      ],
      "metadata": {
        "id": "0W9IOu9WuYj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_Model = LogisticRegression(random_state=1)\n",
        "lr_Model.fit(X_train, y_train)\n",
        "y_pred = lr_Model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))\n",
        "\n",
        "# Okee-doke, we seem to have hit an end when it comes to Logistic Regression. it gives us almost the same model as we got with the Neural network.\n",
        "#Let's note that logistic regression is deemed as one of the best models for the Titanic dataset. I wouldn't bother with SVM because the data isn't 'short and fat'\n",
        "\n"
      ],
      "metadata": {
        "id": "FYpWOJt7ugAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the LAST model I want to bother with is kNN.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_Model = KNeighborsClassifier(7)\n",
        "knn_Model.fit(X_train, y_train)\n",
        "\n",
        "y_pred2 = knn_Model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred2, y_test))\n"
      ],
      "metadata": {
        "id": "GEKUsAaRw3Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concluding remarks:**\n",
        "\n",
        "While we saw that the accuracy hit an 77 per cent in the Logistic Regression model, that is the accuracy I would like to go with, for now. However, I have **just** about made it. - This varied with multiple attempts. The highest I got in a logistic regression for this dataset was an 81 percent, 82 for KNN with 5 neighbours.\n",
        "\n",
        "It is important to know that the dataset is imbalanced, with more people in class 'not survived', and less in 'survived'. Moreover, if I had more time, I would have used VIF (Variance inflation factor) to check if I could find more multicollinearity.\n",
        "\n",
        "-Advaith Subramanian Sahasranamam"
      ],
      "metadata": {
        "id": "_4Brm7sKx7UE"
      }
    }
  ]
}