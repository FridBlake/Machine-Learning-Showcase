{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMo+x5g3Gv6TmzYD42+g9UD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FridBlake/Machine-Learning-Showcase/blob/main/Titanic%20Dataset%20From%20Kaggle%20-%20testing%20out%20GridSearchCV%20and%20preprocessing%20tools.\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rqBQQlCdUs4"
      },
      "outputs": [],
      "source": [
        "#these two modules will help us to graph the data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#pandas will help us visualise the data and manipulate it so that we can clean and treat it.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#put this on whenver you use matplotlib\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we are strong a PANDAS object of type 'DataFrame' in a variable data, which will use a .csv file.\n",
        "data = pd.read_csv('train.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "n8mvrXoNeKZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Survived'].value_counts()"
      ],
      "metadata": {
        "id": "cLZyp0wW9cn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we only have 342 survivors compared to 549, we have two options. We can either undersample the data (remove data of 207 people who did not survive) - but that will leave us with smaller number of instances to work with. So, we OVERSAMPLE our dataset.\n",
        "\n",
        "This means we choose random instances of class 1, and duplicate them. This will give the program a better chance to understand what's really going on. I will do this after we split our data."
      ],
      "metadata": {
        "id": "WwFZbjuv9nyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation\n"
      ],
      "metadata": {
        "id": "aYoCtkDUesz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#process by which missing values are replaced with the most likely ones.\n",
        "\n",
        "#.isnull() shows you a DataFrame, with T/F to show if the data is missing. .sum() shows you the sum for all missing data in a column.\n",
        "data.isnull().sum()\n",
        "\n",
        "#when you want to isolate a single column, use data[['Column name here']]. .median() finds the median of data the column.\n",
        "medianage = data['Age'].median()\n",
        "\n",
        "#.fillna() fills the missing data, taking in the 1st parameter as what you want to replace the data with, and inplace=True, if you want the original data variable to be modified.\n",
        "data['Age'].fillna(medianage, inplace=True)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "UY4ALWMDepR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in enumerate(['SibSp', 'Parch', 'Age']):\n",
        "  plt.figure(i)\n",
        "  sns.catplot(x=j, y='Survived', data= data, kind='point', aspect=2)"
      ],
      "metadata": {
        "id": "ewgaTWbljSUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''since we now see that there is a relationship (inverse) beteween the number of family members they have in total and the survival rate,\n",
        "we can go ahead and add them up into one column, so as to prevent collinearity. - Plus, we also know that age is not really following a pattern at all.'''\n",
        "\n",
        "data['fam_ct'] = data['SibSp'] + data['Parch']\n",
        "\n",
        "#now we can just go ahead and get rid of SibSp and PARCH.\n",
        "\n",
        "data.drop('SibSp', axis=1, inplace=True)\n",
        "data.drop('Parch', axis=1, inplace=True)\n",
        "data.drop('PassengerId', axis=1, inplace=True)\n",
        "\n",
        "data\n"
      ],
      "metadata": {
        "id": "-xtCYhuK2rPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#To go one step ahead, I will plot a pie chart and a few histograms to represent the data.\n",
        "plt.pie(data[data['Survived'] == 1]['Sex'].value_counts(), labels=['Female', 'Male'], explode=(0,0.1))\n",
        "plt.show()\n",
        "\n",
        "#we see a correlation here, with more females surviving than males."
      ],
      "metadata": {
        "id": "N8X9KXVRTEVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to verify this, I will call a simple groupby function.\n",
        "\n",
        "data.groupby(data['Survived'])['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "Zkjg4_poaQ_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLOTTING THE DISTRUBUTION OF FAM_CT WITH SURVIVAL\n",
        "%matplotlib inline\n",
        "\n",
        "plt.hist(data[data['Survived']==1]['fam_ct'], color = 'beige', edgecolor = 'black', density=False, label='familyCount')\n",
        "plt.show()\n",
        "print('Family count vs survival')\n",
        "plt.hist(data[data['Survived']==1]['Age'], color = 'lime', edgecolor='black', density=False, label='age')\n",
        "plt.show()\n",
        "print('Age vs survival')\n"
      ],
      "metadata": {
        "id": "OUZbqSO5Wsu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.groupby(data['Cabin'].isnull())['Survived'].mean()\n",
        "data['Cabin'].isnull().value_counts()\n",
        "\n",
        "#clearly, we see that people who have not had a cabin are more likely to die. "
      ],
      "metadata": {
        "id": "e3C1kdEzbeeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I would like to go ahead and work with the categorical variables, and change them."
      ],
      "metadata": {
        "id": "F5WSTgTu3ZmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I replace sex with binary numbers.\n",
        "data['Sex'] = data['Sex'].map({'male' : 0, 'female' : 1})\n",
        "\n",
        "#if they don't have a cabin, it's a 0, else it's a 1. - numpy has this helpful function called np.where() - which will do the mapping for us.\n",
        "data[['hasCabin']] = np.where(data[['Cabin']].isnull(), 0, 1)\n",
        "data.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "#now I would like to do the same thing for data['Embarked'] and see if it makes a difference at all.\n",
        "\n"
      ],
      "metadata": {
        "id": "8blb_B2L3gfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The place where they embarked did NOT make too much of a difference, so I drop it.\n",
        "#So I drop the ones that we do not need.\n",
        "data.drop('Embarked', axis=1, inplace=True)\n",
        "data.drop('Name', axis=1, inplace=True)\n",
        "data.drop('Ticket', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "cJF5e5RY7rSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data\n",
        "#this is what we are left with now."
      ],
      "metadata": {
        "id": "CcIkPEOB77e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the fact that we have reduced out dimensions from 12 to 7 will reduce the risk of curse \n",
        "#of dimensionality hurting the model.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data[['Survived']]\n",
        "\n",
        "cNames = X.columns\n",
        "\n",
        "#data has now been normalised - minMaxScaler() shifts data from 0 to 1.\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=cNames)\n",
        "X\n",
        "y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "#now 30 percent of the data is in testing group."
      ],
      "metadata": {
        "id": "WwxW2oYf8zHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "ZihLvlhSkb_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us first try out a few different classification methods, and then go with the best.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "RC_model = RandomForestClassifier()\n",
        "\n",
        "parameters = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 'max_depth' : [1, 2, 3, 4, 5, 6]}\n",
        "\n",
        "cv = GridSearchCV(RC_model, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "95gpcY_ik1dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.best_estimator_\n",
        "#with this we can find that this random forest classifier works best at a max depth of 6 and n_est = 20."
      ],
      "metadata": {
        "id": "BmF90BQ-mlqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The random forst model has an accuracy of 87 percent.\n",
        "\n",
        "rf_Model = RandomForestClassifier(max_depth = 6, n_estimators = 20)\n",
        "rf_Model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_Model.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "8LI7Ck3Pmogh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Now, let us try a multilayer perceptron.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "nn_Model = keras.Sequential([\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cb = keras.callbacks.EarlyStopping(min_delta = 0.01, patience=3)\n",
        "\n",
        "nn_Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = nn_Model.fit(X_train, y_train, epochs=200, callbacks = cb, validation_data=(X_test, y_test), batch_size=16)\n",
        "\n",
        "plt.plot(history.history['loss'], color='blue')\n",
        "plt.plot(history.history['val_loss'], color='yellow')\n",
        "plt.show()\n",
        "\n",
        "y_pred = nn_Model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "haqFuURZnbNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "now that is only as far as i can get it to go. Neural networks are rather notorious to train indeed.\n",
        "After trying out an MLP package from SKLEARN, I want to try naive_bayes, GaussianNB to be specific. - Again, Gaussian NB turns out to be terrible in this case.\n",
        "'''"
      ],
      "metadata": {
        "id": "ElqblyU7rADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn2 = MLPClassifier()\n",
        "parameters = {'hidden_layer_sizes' : [2, 3, 4, 5, 6, 7, 8, 9], 'early_stopping' : [True, False], 'activation' : ['identity', 'tanh', 'logistic', 'relu']}\n",
        "cv2 = GridSearchCV(nn2, parameters, cv=5)\n",
        "\n",
        "cv2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OwwIbwGDgSuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the best estimator params.\n",
        "cv2.best_estimator_\n",
        "\n",
        "bestestimatornn = MLPClassifier(activation='relu', hidden_layer_sizes=5, early_stopping = True)\n",
        "bestestimatornn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5kRvmUbZiP5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#despite some randomness, \n",
        "y_pred = bestestimatornn.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "aUOd2IG2ivSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_test, y_test)\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))\n"
      ],
      "metadata": {
        "id": "O6x4ybjwrTgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try out logistic regression - used for data that's rather 'behaved' - in other words, it's not made of any outliers.\n"
      ],
      "metadata": {
        "id": "0W9IOu9WuYj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_Model = LogisticRegression(random_state=1)\n",
        "lr_Model.fit(X_train, y_train)\n",
        "y_pred = lr_Model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))\n",
        "\n",
        "# Okee-doke, we seem to have hit an end when it comes to Logistic Regression. it gives us almost the same model as we got with the Neural network.\n",
        "#Let's note that logistic regression is deemed as one of the best models for the Titanic dataset. I wouldn't bother with SVM because the data isn't 'short and fat'\n",
        "\n"
      ],
      "metadata": {
        "id": "FYpWOJt7ugAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the LAST model I want to bother with is kNN.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_Model = KNeighborsClassifier()\n",
        "parameters = {'n_neighbors' : [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}\n",
        "cv3 = GridSearchCV(knn_Model, parameters, cv=5)\n",
        "cv3.fit(X_test, y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GEKUsAaRw3Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv3.best_estimator_"
      ],
      "metadata": {
        "id": "kFldk4dBjrhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_model = KNeighborsClassifier(n_neighbors = 9)\n",
        "_model.fit(X_train, y_train)\n",
        "y_pred = _model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "bzsTCKQdkNvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concluding remarks:**\n",
        "\n",
        "While we saw that the accuracy hit an 81 per cent in the Logistic Regression model, that is the accuracy I would like to go with, for now. However, I have **just** about made it. - This varied with multiple attempts. The highest I got in a logistic regression for this dataset was an 81 percent, 82 for KNN with 5 neighbours.\n",
        "\n",
        "It is important to know that the dataset is imbalanced, with more people in class 'not survived', and less in 'survived'. Moreover, if I had more time, I would have used VIF (Variance inflation factor) to check if I could find more multicollinearity.\n",
        "\n",
        "Remember:\n",
        "\n",
        "Data science is 80 percent preparing data, 20 percent complaining about preparing data. **bold text**\n",
        "\n",
        "-Advaith Subramanian Sahasranamam"
      ],
      "metadata": {
        "id": "_4Brm7sKx7UE"
      }
    }
  ]
}